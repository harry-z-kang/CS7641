The analysis on the figure for each dataset is included within the respective Jupyter Notebook. In this README, I will mainly compare between the two datasets.

First of all, the breast cancer dataset is much smaller than the dry bean dataset. The breast cancer dataset has 569 instances and 30 features whereas the dry bean dataset has 13611 instances and 17 features. The breast cancer dataset is also much more balanced than the dry bean dataset. The breast cancer dataset has 357 benign instances and 212 malignant instances whereas the dry bean dataset has 6757 instances of class 1, 3659 instances of class 2, 1155 instances of class 3, 1155 instances of class 4, 1155 instances of class 5, 1155 instances of class 6, and 1155 instances of class 7.

The dramatic size difference can be easily observed from the training time and inference time. Training the SVM for the dry bean dataset takes as long as the time to go through the whole notebook for breast cancer dataset.

The dry bean dataset is a multi-classification whereas the breast cancer dataset is a binary classification problem. This meant that the dry bean dataset will requre the classifier to be more resilient to noise and may need a much more complex model to classify the data. This is quite obvious for the boosting classifier as the weak learner for the breast cancer dataset is just a simple one leaf decision tree whereas the weak learner for the dry bean dataset has a maximum dpeth of 3.

For the neural network, I looks like dataset with less instances are more sensitive to noise. This is reflected from the loss curve & score curve. For the breast cancer dataset, the score curve oscillates between 92% and 98% whereas for the dy bean dataset, the score curve is fairly smooth. This is also reflected from the loss curve. The loss curve for the breast cancer dataset is much more noisy than the loss curve for the dry bean dataset. In another with more instances, it's less likely to overfit. Adding more examples, adds diversity. It decreases the generalization error because your model becomes more general by virtue of being trained on more examples.
